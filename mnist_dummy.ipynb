{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4gYazJBAPPM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "#from torchsummary import summary\n",
    "import time\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bM3MY6BCAPPY"
   },
   "outputs": [],
   "source": [
    "# download and transform train dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# download and transform test dataset\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eouopv9YAPPh"
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"Custom module for a simple convnet classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(10, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input is 28x28x1\n",
    "        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n",
    "        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n",
    "        \n",
    "        # Do not be afraid of F's - those are just functional wrappers for modules form nn package\n",
    "        # Please, see for yourself - http://pytorch.org/docs/_modules/torch/nn/functional.html\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # conv2(kernel=5, filters=20) 12x12x20 -> 8x8x20\n",
    "        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv2(x), 8))\n",
    "        \n",
    "        # flatten 4x4x20 = 320\n",
    "        x = x.view(-1, 10)\n",
    "        \n",
    "        # 320 -> 50\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # transform to logits\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdaEJm3EAPPq"
   },
   "outputs": [],
   "source": [
    "# create classifier and optimizer objects\n",
    "clf = CNNClassifier().to(device)\n",
    "opt = optim.SGD(clf.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss().to(device)\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "def train(epoch):\n",
    "    clf.train() # set model in training mode (need this because of dropout)\n",
    "    \n",
    "    # dataset API gives us pythonic batching \n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        data = Variable(data).to(device)\n",
    "        target = Variable(label).to(device)\n",
    "        \n",
    "        # forward pass, calculate loss and backprop!\n",
    "        opt.zero_grad()\n",
    "        preds = clf(data)\n",
    "        loss = criterion(preds, target)\n",
    "        loss.backward()\n",
    "        loss_history.append(loss.item())\n",
    "        opt.step()\n",
    "        \n",
    "        if batch_id % 1000 == 0:\n",
    "            print(loss.item())\n",
    "\n",
    "def test(epoch):\n",
    "    with torch.no_grad():\n",
    "        clf.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = clf(data)\n",
    "            test_loss += F.nll_loss(output, target).item()\n",
    "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CNNClassifier(\n",
       "    (conv1): Conv2d(1, 5, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (conv2): Conv2d(5, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (fc1): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        clf = nn.DataParallel(clf)\n",
    "\n",
    "clf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "qvjEplW_APPv",
    "outputId": "8acd8180-640f-4684-f713-ef18be48e31c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "2.2886264324188232\n",
      "0.13259916007518768\n",
      "0.06747128814458847\n",
      "0.3153684139251709\n",
      "0.10231061279773712\n",
      "0.012948465533554554\n",
      "\n",
      "Test set: Average loss: 0.1877, Accuracy: 9477/10000 (94%)\n",
      "\n",
      "Epoch 1\n",
      "0.03583803027868271\n",
      "0.09544344246387482\n",
      "0.0319838747382164\n",
      "0.42813271284103394\n",
      "0.04897008091211319\n",
      "0.07742156833410263\n",
      "\n",
      "Test set: Average loss: 0.1543, Accuracy: 9544/10000 (95%)\n",
      "\n",
      "Epoch 2\n",
      "0.03315744549036026\n",
      "0.028505325317382812\n",
      "0.009849119000136852\n",
      "0.08729724586009979\n",
      "0.07444243133068085\n",
      "0.008196592330932617\n",
      "\n",
      "Test set: Average loss: 0.1349, Accuracy: 9590/10000 (95%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0,3):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zzpuh0x-WnDs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5P0D97boUb5R"
   },
   "outputs": [],
   "source": [
    "def eval_hessian(loss_grad, model):\n",
    "    cnt = 0\n",
    "    for g in loss_grad:\n",
    "        g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "        cnt = 1\n",
    "    l = g_vector.size(0)\n",
    "    g_vector = g_vector.to(device)\n",
    "    hessian = torch.zeros(l, l)\n",
    "    for idx in range(l):\n",
    "        grad2rd = torch.autograd.grad(g_vector[idx], model.parameters(), create_graph=True)\n",
    "        cnt = 0\n",
    "        for g in grad2rd:\n",
    "            g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
    "            cnt = 1\n",
    "        hessian[idx] = g2\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_id, (data, label) in enumerate(train_loader):\n",
    "    data = Variable(data).to(device)\n",
    "    target = Variable(label).to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple qudratic form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1.],\n",
      "        [1., 2.]], requires_grad=True)\n",
      "tensor([[2., 1.],\n",
      "        [1., 2.]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [2.,1.],\n",
    "    [1.,2.]\n",
    "], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([2., 3.], requires_grad=True)\n",
    "\n",
    "f = .5*x@A@x\n",
    "\n",
    "loss_grad, = torch.autograd.grad(f, x, create_graph=True)\n",
    "\n",
    "cnt = 0\n",
    "for g in loss_grad:\n",
    "    g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "    cnt = 1\n",
    "l = g_vector.size(0)\n",
    "g_vector = g_vector.to(device)\n",
    "hessian = torch.zeros(l, l)\n",
    "for idx in range(l):\n",
    "    grad2rd = torch.autograd.grad(g_vector[idx], x, create_graph=True)\n",
    "    cnt = 0\n",
    "    for g in grad2rd:\n",
    "        g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
    "        cnt = 1\n",
    "    hessian[idx] = g2\n",
    "    \n",
    "print(A)\n",
    "print(hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liner regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "nb = 10\n",
    "y = np.random.rand(N)\n",
    "X = np.random.rand(N, nb)*2-1\n",
    "w = np.random.rand(nb)\n",
    "#b = np.random.rand(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, requires_grad=True)\n",
    "w = torch.tensor(w, requires_grad=True)\n",
    "#b = torch.tensor(b, requires_grad=True)\n",
    "y = torch.tensor(y, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.norm(X@w-y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad, = torch.autograd.grad(loss, w, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for g in loss_grad:\n",
    "    g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "    cnt = 1\n",
    "l = g_vector.size(0)\n",
    "g_vector = g_vector.to(device)\n",
    "hessian = torch.zeros(l, l)\n",
    "for idx in range(l):\n",
    "    grad2rd = torch.autograd.grad(g_vector[idx], w, create_graph=True)\n",
    "    cnt = 0\n",
    "    for g in grad2rd:\n",
    "        g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
    "        cnt = 1\n",
    "    hessian[idx] = g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(2*X.detach().numpy().T@X.detach().numpy(), hessian.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary logistic regression\n",
    "#https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/lecture6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "X = np.random.rand(10,2)*2-1\n",
    "w = np.array([2., 3.])\n",
    "y = (sigmoid(X@w+np.random.rand())>.5).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, requires_grad=True)\n",
    "w = torch.tensor(w, requires_grad=True)\n",
    "#b = torch.tensor(b, requires_grad=True)\n",
    "y = torch.tensor(y, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/shared/opt/python-3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "#binary cross entropy\n",
    "loss = -torch.sum(\n",
    "    torch.log(F.sigmoid(X@w))*y + \n",
    "    torch.log(1-F.sigmoid(X@w))*(1-y)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8753, -1.0963], dtype=torch.float64, grad_fn=<MvBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = F.sigmoid(X@w)\n",
    "torch.transpose(X, 1, 0) @(pi-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad, = torch.autograd.grad(loss, w, create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8753, -1.0963], dtype=torch.float64, grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for g in loss_grad:\n",
    "    g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "    cnt = 1\n",
    "l = g_vector.size(0)\n",
    "g_vector = g_vector.to(device)\n",
    "hessian = torch.zeros(l, l)\n",
    "for idx in range(l):\n",
    "    grad2rd = torch.autograd.grad(g_vector[idx], w, create_graph=True)\n",
    "    cnt = 0\n",
    "    for g in grad2rd:\n",
    "        g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
    "        cnt = 1\n",
    "    hessian[idx] = g2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6700, -0.3953],\n",
       "        [-0.3953,  0.4963]], dtype=torch.float64, grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = F.sigmoid(X@w)\n",
    "torch.transpose(X, 1, 0) @ torch.diag(pi*(1-pi)) @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6700, -0.3953],\n",
       "        [-0.3953,  0.4963]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(2, 2, bias=False)\n",
    "    def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(10,2)*2-1\n",
    "w = np.array([2., 3.])\n",
    "y = (sigmoid(X@w+np.random.rand()/10)>.5).astype('float')\n",
    "\n",
    "X = Variable(torch.tensor(X).float())\n",
    "#w = torch.tensor([w], requires_grad=True).type(torch.float32)\n",
    "#b = torch.tensor(b, requires_grad=True)\n",
    "y = Variable(torch.tensor(y).long())\n",
    "\n",
    "model = LogisticRegression()\n",
    "#model.linear.weight = nn.Parameter(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "mnist_dummy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
